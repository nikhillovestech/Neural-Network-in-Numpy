{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "8bec90e6-23f2-4aae-8b4a-e9ef999391b7",
    "_uuid": "ae07d14dcb75d8188c49d57cdc54fc36f3397d02"
   },
   "outputs": [],
   "source": [
    "#importing all the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets and Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('wine.csv')\n",
    "# print(df)\n",
    "a = pd.get_dummies(df['Wine'])\n",
    "df = pd.concat([df,a],axis=1)\n",
    "X = df.drop([1, 2,3,'Wine'], axis = 1)\n",
    "y = df[[1,2,3]].values\n",
    "X_train, X_test, Y_train,Y_test = train_test_split(X, y, test_size=0.20,)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# Y_test,test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wine</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic.acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Acl</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid.phenols</th>\n",
       "      <th>Proanth</th>\n",
       "      <th>Color.int</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD</th>\n",
       "      <th>Proline</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Wine  Alcohol  Malic.acid   Ash   Acl   Mg  Phenols  Flavanoids  \\\n",
       "0     1    14.23        1.71  2.43  15.6  127     2.80        3.06   \n",
       "1     1    13.20        1.78  2.14  11.2  100     2.65        2.76   \n",
       "2     1    13.16        2.36  2.67  18.6  101     2.80        3.24   \n",
       "3     1    14.37        1.95  2.50  16.8  113     3.85        3.49   \n",
       "4     1    13.24        2.59  2.87  21.0  118     2.80        2.69   \n",
       "\n",
       "   Nonflavanoid.phenols  Proanth  Color.int   Hue    OD  Proline  1  2  3  \n",
       "0                  0.28     2.29       5.64  1.04  3.92     1065  1  0  0  \n",
       "1                  0.26     1.28       4.38  1.05  3.40     1050  1  0  0  \n",
       "2                  0.30     2.81       5.68  1.03  3.17     1185  1  0  0  \n",
       "3                  0.24     2.18       7.80  0.86  3.45     1480  1  0  0  \n",
       "4                  0.39     1.82       4.32  1.04  2.93      735  1  0  0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wine</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic.acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Acl</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid.phenols</th>\n",
       "      <th>Proanth</th>\n",
       "      <th>Color.int</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD</th>\n",
       "      <th>Proline</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>3</td>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>3</td>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>3</td>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>3</td>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>3</td>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Wine  Alcohol  Malic.acid   Ash   Acl   Mg  Phenols  Flavanoids  \\\n",
       "173     3    13.71        5.65  2.45  20.5   95     1.68        0.61   \n",
       "174     3    13.40        3.91  2.48  23.0  102     1.80        0.75   \n",
       "175     3    13.27        4.28  2.26  20.0  120     1.59        0.69   \n",
       "176     3    13.17        2.59  2.37  20.0  120     1.65        0.68   \n",
       "177     3    14.13        4.10  2.74  24.5   96     2.05        0.76   \n",
       "\n",
       "     Nonflavanoid.phenols  Proanth  Color.int   Hue    OD  Proline  1  2  3  \n",
       "173                  0.52     1.06        7.7  0.64  1.74      740  0  0  1  \n",
       "174                  0.43     1.41        7.3  0.70  1.56      750  0  0  1  \n",
       "175                  0.43     1.35       10.2  0.59  1.56      835  0  0  1  \n",
       "176                  0.53     1.46        9.3  0.60  1.62      840  0  0  1  \n",
       "177                  0.56     1.35        9.2  0.61  1.60      560  0  0  1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(model,a0):\n",
    "    \n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3']\n",
    "    # Do the first Linear step \n",
    "    # Z1 is the input layer x times the dot product of the weights + bias b\n",
    "    z1 = a0.dot(W1) + b1\n",
    "    \n",
    "    # Put it through the first activation function\n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    # Second linear step\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    \n",
    "    # Second activation function\n",
    "    a2 = np.tanh(z2)\n",
    "    \n",
    "    #Third linear step\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    \n",
    "    #For the Third linear activation function we use the softmax function, either the sigmoid of softmax should be used for the last layer\n",
    "    a3 = softmax(z3)\n",
    "    \n",
    "    #Store all results in these values\n",
    "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}\n",
    "    return cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    #Calculate exponent term first\n",
    "    exp_scores = np.exp(z)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(model,cache,y):\n",
    "\n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'],model['W3'],model['b3']\n",
    "    \n",
    "    # Load forward propagation results\n",
    "    a0,a1, a2,a3 = cache['a0'],cache['a1'],cache['a2'],cache['a3']\n",
    "    \n",
    "    # Get number of samples\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    # Calculate loss derivative with respect to output\n",
    "    dz3 = loss_derivative(y=y,y_hat=a3)\n",
    "\n",
    "    # Calculate loss derivative with respect to second layer weights\n",
    "    dW3 = 1/m*(a2.T).dot(dz3) #dW2 = 1/m*(a1.T).dot(dz2) \n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer bias\n",
    "    db3 = 1/m*np.sum(dz3, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer\n",
    "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer weights\n",
    "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer bias\n",
    "    db2 = 1/m*np.sum(dz2, axis=0)\n",
    "    \n",
    "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
    "    \n",
    "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
    "    \n",
    "    db1 = 1/m*np.sum(dz1,axis=0)\n",
    "    \n",
    "    # Store gradients\n",
    "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss/Objective/Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(y,y_hat):\n",
    "    # Clipping value\n",
    "    minval = 0.000000000001\n",
    "    # Number of samples\n",
    "    m = y.shape[0]\n",
    "    # Loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula\n",
    "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and activation derivative for backpropagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_derivative(y,y_hat):\n",
    "    return (y_hat-y)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - np.power(x, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly initialize all Neural Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(nn_input_dim,nn_hdim,nn_output_dim):\n",
    "    # First layer weights\n",
    "    W1 = 2 *np.random.randn(nn_input_dim, nn_hdim) - 1\n",
    "    \n",
    "    # First layer bias\n",
    "    b1 = np.zeros((1, nn_hdim))\n",
    "    \n",
    "    # Second layer weights\n",
    "    W2 = 2 * np.random.randn(nn_hdim, nn_hdim) - 1\n",
    "    \n",
    "    # Second layer bias\n",
    "    b2 = np.zeros((1, nn_hdim))\n",
    "    W3 = 2 * np.random.rand(nn_hdim, nn_output_dim) - 1\n",
    "    b3 = np.zeros((1,nn_output_dim))\n",
    "    \n",
    "    \n",
    "    # Package and return model\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3}\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(model,grads,learning_rate):\n",
    "    # Load parameters\n",
    "    W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
    "    \n",
    "    # Update parameters\n",
    "    W1 -= learning_rate * grads['dW1']\n",
    "    b1 -= learning_rate * grads['db1']\n",
    "    W2 -= learning_rate * grads['dW2']\n",
    "    b2 -= learning_rate * grads['db2']\n",
    "    W3 -= learning_rate * grads['dW3']\n",
    "    b3 -= learning_rate * grads['db3']\n",
    "    \n",
    "    # Store and return parameters\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3':W3,'b3':b3}\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x):\n",
    "    # Do forward pass\n",
    "    c = forward_prop(model,x)\n",
    "    #get y_hat\n",
    "    y_hat = np.argmax(c['a3'], axis=1)\n",
    "    return y_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,X_,y_,learning_rate, epochs, print_loss=False):\n",
    "    # Gradient descent. Loop over epochs\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        cache = forward_prop(model,X_)\n",
    "\n",
    "        # Backpropagation\n",
    "        grads = backward_prop(model,cache,y_)\n",
    "        \n",
    "        # Gradient descent parameter update\n",
    "        # Assign new parameters to the model\n",
    "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
    "    \n",
    "        # Pring loss & accuracy every 100 iterations\n",
    "        if print_loss and i % 100 == 0:\n",
    "            a3 = cache['a3']\n",
    "            print('Loss after iteration',i,':',softmax_loss(y_,a3))\n",
    "            y_hat = predict(model,X_)\n",
    "            y_true = y_.argmax(axis=1)\n",
    "            print('Accuracy after iteration',i,':',accuracy_score(y_pred=y_hat,y_true=y_true)*100,'%')\n",
    "            losses.append(accuracy_score(y_pred=y_hat,y_true=y_true)*100)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model parameters and train model on wine dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "90023c7a-8059-4117-98b3-82ceb3e5877a",
    "_uuid": "de660f4b64992b03558fc14ee176479b4b7afc29",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 : 1.2100415957260722\n",
      "Accuracy after iteration 0 : 28.169014084507044 %\n",
      "Loss after iteration 100 : 0.6256966184797119\n",
      "Accuracy after iteration 100 : 75.35211267605634 %\n",
      "Loss after iteration 200 : 0.4972699650135179\n",
      "Accuracy after iteration 200 : 80.98591549295774 %\n",
      "Loss after iteration 300 : 0.4048276431647925\n",
      "Accuracy after iteration 300 : 85.2112676056338 %\n",
      "Loss after iteration 400 : 0.31421485064394844\n",
      "Accuracy after iteration 400 : 90.84507042253522 %\n",
      "Loss after iteration 500 : 0.27012039225408513\n",
      "Accuracy after iteration 500 : 90.84507042253522 %\n",
      "Loss after iteration 600 : 0.23962937565157977\n",
      "Accuracy after iteration 600 : 92.25352112676056 %\n",
      "Loss after iteration 700 : 0.21792142605926387\n",
      "Accuracy after iteration 700 : 92.95774647887323 %\n",
      "Loss after iteration 800 : 0.20642748691915402\n",
      "Accuracy after iteration 800 : 92.95774647887323 %\n",
      "Loss after iteration 900 : 0.19810383137185023\n",
      "Accuracy after iteration 900 : 92.95774647887323 %\n",
      "Loss after iteration 1000 : 0.19043506362518034\n",
      "Accuracy after iteration 1000 : 93.66197183098592 %\n",
      "Loss after iteration 1100 : 0.18352022190762007\n",
      "Accuracy after iteration 1100 : 93.66197183098592 %\n",
      "Loss after iteration 1200 : 0.17833993868477369\n",
      "Accuracy after iteration 1200 : 93.66197183098592 %\n",
      "Loss after iteration 1300 : 0.1744502847377467\n",
      "Accuracy after iteration 1300 : 93.66197183098592 %\n",
      "Loss after iteration 1400 : 0.1713760056842125\n",
      "Accuracy after iteration 1400 : 93.66197183098592 %\n",
      "Loss after iteration 1500 : 0.16879946362256174\n",
      "Accuracy after iteration 1500 : 93.66197183098592 %\n",
      "Loss after iteration 1600 : 0.16657520051414426\n",
      "Accuracy after iteration 1600 : 94.36619718309859 %\n",
      "Loss after iteration 1700 : 0.1646066865188765\n",
      "Accuracy after iteration 1700 : 94.36619718309859 %\n",
      "Loss after iteration 1800 : 0.1628211129681701\n",
      "Accuracy after iteration 1800 : 94.36619718309859 %\n",
      "Loss after iteration 1900 : 0.16116514745234062\n",
      "Accuracy after iteration 1900 : 94.36619718309859 %\n",
      "Loss after iteration 2000 : 0.15959631329697768\n",
      "Accuracy after iteration 2000 : 94.36619718309859 %\n",
      "Loss after iteration 2100 : 0.15807669563467855\n",
      "Accuracy after iteration 2100 : 94.36619718309859 %\n",
      "Loss after iteration 2200 : 0.15657426911025035\n",
      "Accuracy after iteration 2200 : 94.36619718309859 %\n",
      "Loss after iteration 2300 : 0.15507023762337538\n",
      "Accuracy after iteration 2300 : 94.36619718309859 %\n",
      "Loss after iteration 2400 : 0.15355683224638234\n",
      "Accuracy after iteration 2400 : 94.36619718309859 %\n",
      "Loss after iteration 2500 : 0.1520218220354194\n",
      "Accuracy after iteration 2500 : 94.36619718309859 %\n",
      "Loss after iteration 2600 : 0.15045366758377346\n",
      "Accuracy after iteration 2600 : 94.36619718309859 %\n",
      "Loss after iteration 2700 : 0.1488813649480597\n",
      "Accuracy after iteration 2700 : 94.36619718309859 %\n",
      "Loss after iteration 2800 : 0.14735800861473272\n",
      "Accuracy after iteration 2800 : 94.36619718309859 %\n",
      "Loss after iteration 2900 : 0.14588881671840784\n",
      "Accuracy after iteration 2900 : 94.36619718309859 %\n",
      "Loss after iteration 3000 : 0.14445438213746103\n",
      "Accuracy after iteration 3000 : 94.36619718309859 %\n",
      "Loss after iteration 3100 : 0.1430372060666907\n",
      "Accuracy after iteration 3100 : 94.36619718309859 %\n",
      "Loss after iteration 3200 : 0.14161607768977233\n",
      "Accuracy after iteration 3200 : 95.07042253521126 %\n",
      "Loss after iteration 3300 : 0.14016327262715664\n",
      "Accuracy after iteration 3300 : 95.07042253521126 %\n",
      "Loss after iteration 3400 : 0.1386434958720706\n",
      "Accuracy after iteration 3400 : 95.07042253521126 %\n",
      "Loss after iteration 3500 : 0.13700773227120727\n",
      "Accuracy after iteration 3500 : 95.07042253521126 %\n",
      "Loss after iteration 3600 : 0.13519993098510458\n",
      "Accuracy after iteration 3600 : 95.07042253521126 %\n",
      "Loss after iteration 3700 : 0.13325225850144412\n",
      "Accuracy after iteration 3700 : 95.07042253521126 %\n",
      "Loss after iteration 3800 : 0.13131600797552412\n",
      "Accuracy after iteration 3800 : 95.77464788732394 %\n",
      "Loss after iteration 3900 : 0.12936194773987963\n",
      "Accuracy after iteration 3900 : 95.77464788732394 %\n",
      "Loss after iteration 4000 : 0.12746673747560167\n",
      "Accuracy after iteration 4000 : 95.77464788732394 %\n",
      "Loss after iteration 4100 : 0.12527363782337195\n",
      "Accuracy after iteration 4100 : 95.77464788732394 %\n",
      "Loss after iteration 4200 : 0.12171447144682866\n",
      "Accuracy after iteration 4200 : 95.77464788732394 %\n",
      "Loss after iteration 4300 : 0.1155578121474375\n",
      "Accuracy after iteration 4300 : 97.1830985915493 %\n",
      "Loss after iteration 4400 : 0.1085188109473492\n",
      "Accuracy after iteration 4400 : 97.1830985915493 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7cb4560610>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = initialize_parameters(nn_input_dim=13, nn_hdim= 5, nn_output_dim= 3)\n",
    "model = train(model,X_train,Y_train,learning_rate=0.07,epochs=4500,print_loss=True)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Calculate testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "cf85a92b-2bc6-4b57-8cd6-53b523add48a",
    "_uuid": "6aca14a72b394b8bc0d83fae7134fd4f8bcbfce2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy is:  83.33333333333334%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test = predict(model,X_test)\n",
    "test = pd.get_dummies(test)\n",
    "Y_test = pd.DataFrame(Y_test)\n",
    "print(\"Testing accuracy is: \",str(accuracy_score(Y_test, test) * 100)+\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
